{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RtQaGf9xK5Gl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import cv2\n",
        "import glob\n",
        "import pathlib\n",
        "import PIL, PIL.Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FkXd94xGNvHS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current directory: /Users/alicialarsen/Documents/TUe/Honors/MedAI/TUe-Honors-MedAI/replicate-BSc-Thesis\n",
            "datasets directory: /Users/alicialarsen/Documents/TUe/Honors/MedAI/TUe-Honors-MedAI/replicate-BSc-Thesis/datasets/exp1\n"
          ]
        }
      ],
      "source": [
        "base_dir = os.getcwd()\n",
        "print(f'current directory: {base_dir}')\n",
        "\n",
        "dataset_dir = pathlib.Path(os.path.join(base_dir, 'datasets/exp1'))\n",
        "print(f'datasets directory: {dataset_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of total images: 1284 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "image_count = len(list(dataset_dir.glob('**/*.png')))\n",
        "print('number of total images:', image_count, '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image resolution: 270x270 pixels\n"
          ]
        }
      ],
      "source": [
        "# Open an arbitrary image file and check it's size\n",
        "image = Image.open(f'{dataset_dir}/state0/img0.png')\n",
        "img_width, img_height = image.size\n",
        "print(f\"Image resolution: {img_width}x{img_height} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Set up tensor data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1284 files belonging to 2 classes.\n",
            "Using 899 files for training.\n",
            "Found 1284 files belonging to 2 classes.\n",
            "Using 385 files for validation.\n",
            "['state0', 'state1']\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  dataset_dir,\n",
        "  validation_split=0.3,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 270, 270, 3)\n",
            "(5,)\n"
          ]
        }
      ],
      "source": [
        "for image_batch, labels_batch in val_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1N4YZkMObNC"
      },
      "source": [
        "### **CNN classification with a VGG-16 model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM9vcz_myyeA",
        "outputId": "70d686e0-40c1-4865-fefc-3551ef0b84e1"
      },
      "outputs": [],
      "source": [
        "def VGG16model(class_names, img_height, img_width):\n",
        "    pass\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    vgg16_model = keras.Sequential()\n",
        "    vgg16_model.add(layers.Conv2D(input_shape=(img_height, img_width, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "    vgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "    vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "    vgg16_model.add(layers.Flatten())\n",
        "    vgg16_model.add(layers.Dense(4096,activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Dense(4096,activation=\"relu\"))\n",
        "    vgg16_model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "    vgg16_model.summary()\n",
        "\n",
        "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
        "    vgg16_model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "    return vgg16_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg16_model = VGG16model(class_names, img_height, img_width)\n",
        "vgg16_model.fit(train_ds, validation_data=val_ds, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CdSSYOS84gcH"
      },
      "outputs": [],
      "source": [
        "models_path = os.path.join(base_dir, 'models')\n",
        "vgg16_model.save(os.path.join(models_path, 'vgg16_model.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M1MmOfj5fHI"
      },
      "source": [
        "use `model = keras.models.load_model('path/to/location')` to load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QDirj4T-M6XW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "vgg16_model = keras.models.load_model(os.path.join(models_path, 'vgg16_model.h5')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vpumo6EwNnq-",
        "outputId": "fb1ba3f3-f894-4011-e090-5fd56b45b6a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(vgg16_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX4yFef0OeVa"
      },
      "source": [
        "### **Evaluations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu8DSB8p5rcg",
        "outputId": "9a03ddf4-a970-44c3-8094-3cad161e1289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 5s 60ms/step - loss: 0.7010 - accuracy: 0.5195\n",
            "simple CNN's accuracy: 51.95%\n",
            "77/77 [==============================] - 43s 563ms/step - loss: 0.6927 - accuracy: 0.5195\n",
            "VGG-16's accuracy: 51.95%\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, name='model'):\n",
        "  loss, acc = model.evaluate(val_ds, batch_size= batch_size)\n",
        "  print(f\"{name}'s accuracy: {round((acc * 100), 2)}%\")\n",
        "\n",
        "evaluate(vgg16_model, 'VGG-16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48DPvcIuIjLa",
        "outputId": "bacc4b71-d0ec-44ae-c0a2-aba171512404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 44s 572ms/step\n",
            "77/77 [==============================] - 5s 61ms/step\n",
            "VGG-16 (predictions, true labels):  [(1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 0)]\n",
            "simple model (predictions, true labels):  [(1, 1), (1, 0), (1, 1), (1, 0), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 0)]\n"
          ]
        }
      ],
      "source": [
        "def predict(model):\n",
        "  return np.argmax(model.predict(val_ds), axis=-1)\n",
        "\n",
        "def get_labels(dataset):\n",
        "  all_val_labels = []\n",
        "  for _, labels in dataset:\n",
        "      all_val_labels.extend(labels.numpy())\n",
        "  all_labels = np.array(all_val_labels)\n",
        "  return all_labels\n",
        "\n",
        "vgg16_predicted = predict(vgg16_model)\n",
        "vgg16_labels = get_labels(val_ds)\n",
        "\n",
        "vgg16_comparison_list = list(zip(predict(vgg16_model), get_labels(val_ds)))\n",
        "print(\"VGG-16 (predictions, true labels): \", vgg16_comparison_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC ROC score: 0.5\n"
          ]
        }
      ],
      "source": [
        "def AUC_curve(curve_type: str):\n",
        "    AUC_CURVE = tf.keras.metrics.AUC(\n",
        "        num_thresholds=200,\n",
        "        curve=curve_type,\n",
        "        summation_method='interpolation',\n",
        "        name=None,\n",
        "        dtype=None,\n",
        "        thresholds=None,\n",
        "        multi_label=False,\n",
        "        num_labels=None,\n",
        "        label_weights=None,\n",
        "        from_logits=False\n",
        "    )\n",
        "    AUC_CURVE.update_state(vgg16_labels, predict(vgg16_model))\n",
        "    AUC_ROC_result = AUC_CURVE.result()\n",
        "    return AUC_CURVE, AUC_ROC_result\n",
        "\n",
        "\n",
        "AUC_ROC, AUC_ROC_result = AUC_curve('ROC')\n",
        "print(\"AUC ROC score:\", AUC_ROC_result.numpy())\n",
        "\n",
        "AUC_PR, AUC_PR_result = AUC_curve('PR')\n",
        "print(\"AUC PR score:\", AUC_PR_result.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notebook by Alicia HH Larsen\n",
        "\n",
        "MedAI, Artificial Intelligence Track, TU/e Honors\n",
        "\n",
        "2024-04-28"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
